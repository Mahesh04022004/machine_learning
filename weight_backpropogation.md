ğŸ”· 1. Multilayer Network (Multilayer Perceptron or MLP)
âœ… What is it?

A Multilayer Neural Network is a stack of multiple layers of neurons:

    One input layer

    One or more hidden layers

    One output layer

Each neuron in one layer is connected to all neurons in the next layer (called fully connected layers).
âœ… Structure:

Input Layer â†’ Hidden Layer(s) â†’ Output Layer

Example:

    Input: Features like height, weight, age

    Hidden layers: Learn complex patterns

    Output: Predict something like "diabetes: yes or no"

âœ… Why "multi-layer" is important?

    A single-layer perceptron can only solve linear problems.

    Multilayer networks with activation functions (like ReLU or sigmoid) can model complex, nonlinear relationships.

ğŸ”· 2. Backpropagation (Learning Algorithm)
âœ… What is it?

Backpropagation is the algorithm that tells the network how to adjust weights and biases after each prediction to reduce the error.

It is based on:

    Gradient Descent

    Chain Rule of Calculus

ğŸ” Step-by-step Overview:

    Forward pass:

        Compute predictions using current weights and biases.

    Compute loss:

        Compare prediction vs actual label using a loss function.

    Backward pass:

        Compute how much each weight contributed to the error.

        Use chain rule to calculate the gradient (partial derivatives).

    Update weights:

        Apply gradient descent to change the weights and biases to reduce error.

ğŸ”£ Formula (simplified):

For weight update:
w=wâˆ’Î±â‹…âˆ‚Lossâˆ‚w
w=wâˆ’Î±â‹…âˆ‚wâˆ‚Lossâ€‹

Where:

    Î±Î± = learning rate

    âˆ‚Lossâˆ‚wâˆ‚wâˆ‚Lossâ€‹ = gradient (from backprop)

ğŸ”„ Analogy:

Think of backpropagation like a teacher correcting a student's mistake:

    The student (model) answers a question (prediction),

    The teacher (loss function) checks it,

    Then gives feedback (gradient),

    The student updates their method (weights) to improve.

ğŸ”· 3. Weight Initialization
âœ… What is it?

Before training begins, weights are usually initialized with random values. But how we initialize them matters a lot for how well the model learns.
ğŸš« Bad Initialization:

    All weights = 0 â†’ All neurons learn the same thing (no learning)

    Large weights â†’ Exploding gradients

    Small weights â†’ Vanishing gradients

âœ… Good Initialization Methods:
Method	Use Case	Description
Random Normal / Uniform	Basic networks	Weights are small random values
Xavier Initialization	Sigmoid/Tanh activations	Keeps variance the same across layers
He Initialization	ReLU activations	Prevents vanishing/exploding gradients
ğŸ”£ He Initialization (for ReLU):
w=N(0,2ninput)
w=N(0,ninputâ€‹2â€‹
â€‹)

This keeps the signal strength stable as it flows through the network.
ğŸ§  Recap Table:
Concept	Simple Definition
Multilayer Network	A deep neural network with input, hidden, and output layers
Backpropagation	Algorithm to learn by updating weights using the error
Weight Initialization	Smart way to start training by assigning weights properly