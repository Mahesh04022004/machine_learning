
âœ… What is Batch Normalization?

Batch Normalization (or BatchNorm) is a technique to make training faster and more stable in neural networks by normalizing the inputs of each layer.
ğŸ§  Simple Definition:

Batch Normalization standardizes the input going into a layer so that:

    The mean is around 0

    The standard deviation is 1

Itâ€™s like cleaning up the data at every layer, not just before the first layer.
ğŸ” Why Do We Need Batch Normalization?

When training deep networks:

    The values (activations) can become too large or too small.

    This makes training unstable, and slow.

ğŸ‘‰ BatchNorm fixes this by keeping the values in a normal range during training.
ğŸ“Š Real-Life Analogy:

Imagine you are baking a cake and using cups to measure flour. If your cups are different sizes each time (one big, one small), the result is inconsistent.

Batch normalization ensures that you always use the same sized cup â†’ it keeps the measurements consistent.
ğŸ“ˆ What It Actually Does (Step-by-Step)

Letâ€™s say you are training a model and you have batch of inputs for a layer:
Step 1: Calculate the Mean
Î¼=1mâˆ‘i=1mxi
Î¼=m1â€‹i=1âˆ‘mâ€‹xiâ€‹
Step 2: Calculate the Standard Deviation
Ïƒ=1mâˆ‘i=1m(xiâˆ’Î¼)2
Ïƒ=m1â€‹i=1âˆ‘mâ€‹(xiâ€‹âˆ’Î¼)2
â€‹
Step 3: Normalize Each Input
x^i=xiâˆ’Î¼Ïƒ
x^iâ€‹=Ïƒxiâ€‹âˆ’Î¼â€‹

Now the inputs have:

    Mean = 0

    Standard Deviation = 1

Step 4: Scale and Shift (Learnable Parameters)
yi=Î³â‹…x^i+Î²
yiâ€‹=Î³â‹…x^iâ€‹+Î²

    Î³ (gamma) and Î² (beta) are learned during training

    They let the model scale or shift the normalized data if needed