ğŸ”· 1. Weights and Bias

These are the building blocks of learning in a neural network.
ğŸ§  Imagine a Neuron:

Each neuron in a neural network receives input, does some math, and passes the result to the next neuron.
This math involves weights and bias.
âœ… Weights (W):

    A weight is like a strength or importance you give to an input.

    It tells the model how much to trust or how much impact each input should have.

    The model learns by adjusting the weights during training.

ğŸ”¹ Example:

Suppose youâ€™re predicting house prices based on:

    Size of the house

    Number of bedrooms

The model might learn:

    Weight for size = 0.8 (important)

    Weight for bedrooms = 0.2 (less important)

âœ… Bias (b):

    Bias is like a starting point or a shift.

    It allows the model to fit the data better, even when the input is zero.

    Without bias, the neuron would always pass through the origin (0,0), which is too restrictive.

ğŸ”¹ Example:

Think of bias like the base salary of a salesperson, and weights are commissions based on performance.
ğŸ”£ Equation (Simple Neuron Output):
Output=(WeightÃ—Input)+Bias
Output=(WeightÃ—Input)+Bias

Then we apply an activation function like ReLU or Sigmoid.
ğŸ”· 2. Loss Function
âœ… What is it?

The loss function tells us how wrong our model is â€” it measures the difference between predicted values and actual values.

The goal of training is to make the loss as small as possible.
ğŸ§  Simple Analogy:

If you're shooting arrows at a target ğŸ¯:

    Your arrows are predictions.

    The bullseye is the true value.

    Loss is how far the arrows land from the bullseye.

âœ… Types of Loss Functions:
Loss Function	Used For	Formula (Simplified)
Mean Squared Error (MSE)	Regression	Average of (actual - predicted)Â²
Mean Absolute Error (MAE)	Regression	Average of
Binary Cross-Entropy	Binary Classification	For probabilities (spam vs not spam)
Categorical Cross-Entropy	Multi-class Classification	For multiple output classes
ğŸ”¹ MSE Example:

Predicted house price = â‚¹95 lakh
Actual house price = â‚¹100 lakh
Error = â‚¹5 lakh
MSE = (5)^2 = 25 (square the error)
ğŸ¯ Goal:

    During training, the model changes the weights and bias to reduce the loss using algorithms like gradient descent.

    When the loss is low, the model is making good predictions.

ğŸ” Relationship Between Them

    Weights and bias are the parameters the model learns.

    The loss function tells us how bad the current parameters are.

    Optimization (like gradient descent) updates weights and bias to minimize the loss.