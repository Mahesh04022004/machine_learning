ðŸ”· What is Linear Regression?

Linear Regression is a supervised learning algorithm used to predict a continuous dependent variable based on one or more independent variables.
âœ… Goal:

    To find the best-fitting straight line that describes the relationship between variables.

ðŸ”· Types of Linear Regression

    Simple Linear Regression

        One independent variable (X)

        Equation:
        Y=mX+c
        Y=mX+c

    Multiple Linear Regression

        Multiple independent variables (Xâ‚, Xâ‚‚, ..., Xâ‚™)

        Equation:
        Y=b0+b1X1+b2X2+â‹¯+bnXn
        Y=b0â€‹+b1â€‹X1â€‹+b2â€‹X2â€‹+â‹¯+bnâ€‹Xnâ€‹

ðŸ”· Components

    Y â€“ Predicted/Target variable (dependent)

    X â€“ Input/Feature variable(s) (independent)

    m or bâ‚ â€“ Slope (change in Y for unit change in X)

    c or bâ‚€ â€“ Intercept (Y value when X=0)

ðŸ”· Cost Function (Loss Function)

We minimize the error between predicted and actual values using:
ðŸ“Œ Mean Squared Error (MSE):
MSE=1nâˆ‘i=1n(yiâˆ’y^i)2
MSE=n1â€‹i=1âˆ‘nâ€‹(yiâ€‹âˆ’y^â€‹iâ€‹)2
ðŸ”· Assumptions of Linear Regression

    Linearity â€“ Relationship between X and Y is linear.

    Independence â€“ Observations are independent.

    Homoscedasticity â€“ Constant variance of errors.

    Normal distribution of residuals (errors).

    No multicollinearity â€“ Independent variables are not highly correlated.

ðŸ”· Advantages

    Simple, fast, and interpretable.

    Performs well when assumptions hold.

ðŸ”· Limitations

    Fails when the relationship is non-linear.

    Sensitive to outliers.

    Assumes constant variance and normality.

ðŸ”· Applications

    Predicting house prices

    Forecasting sales

    Estimating salary based on experience

    Predicting temperature

ðŸ”´ LOGISTIC REGRESSION â€“ Detailed Explanation
ðŸ”· What is Logistic Regression?

Logistic Regression is a supervised learning algorithm used for classification tasks (not regression) â€” particularly binary classification.
âœ… Goal:

    To predict the probability that a given input belongs to a particular class (0 or 1).

ðŸ”· Core Idea

Instead of fitting a straight line, logistic regression fits an S-shaped (sigmoid) curve that maps any real-valued number into a value between 0 and 1.
ðŸ“Œ Sigmoid Function:
P(Y=1âˆ£X)=11+eâˆ’(b0+b1X)
P(Y=1âˆ£X)=1+eâˆ’(b0â€‹+b1â€‹X)1â€‹

    Output is a probability (between 0 and 1)

    If P > 0.5 â†’ Class = 1

    If P < 0.5 â†’ Class = 0

ðŸ”· Types of Logistic Regression

    Binary Logistic Regression â€“ Classify into 2 classes (e.g., pass/fail)

    Multinomial Logistic Regression â€“ More than 2 classes (e.g., low, medium, high)

    Ordinal Logistic Regression â€“ Classes with a natural order (e.g., bad < good < excellent)

ðŸ”· Cost Function for Logistic Regression

We cannot use MSE here; we use Log Loss (Binary Cross Entropy):
Loss=âˆ’1nâˆ‘i=1n[yilogâ¡(y^i)+(1âˆ’yi)logâ¡(1âˆ’y^i)]
Loss=âˆ’n1â€‹i=1âˆ‘nâ€‹[yiâ€‹log(y^â€‹iâ€‹)+(1âˆ’yiâ€‹)log(1âˆ’y^â€‹iâ€‹)]
ðŸ”· Assumptions of Logistic Regression

    Binary/Multiclass output

    No multicollinearity among independent variables

    Large sample size helps model better

    Independent observations

ðŸ”· Advantages

    Good for binary classification

    Outputs probabilities

    Interpretable and efficient
    