âœ… 1. Dropout
ğŸ” What is it?

Dropout is a technique where random neurons are ignored ("dropped") during training.

It means during each training step:

    Some neurons are turned off (their output is set to 0)

    This changes the structure of the network slightly every time

ğŸ¯ Purpose:

To prevent overfitting by not letting the model rely too much on specific neurons.
ğŸ§  Simple Analogy:

Imagine a group of students solving problems. If the same 2-3 students always answer, others wonâ€™t learn.

So the teacher randomly asks different students each time â€” this forces all students to participate. Thatâ€™s dropout in action.
âš™ï¸ How it works:

Letâ€™s say a layer has 100 neurons.

    Dropout rate = 0.2 (20%)

    Then, 20 neurons are randomly disabled during each training iteration

    During testing, all neurons are used, but their outputs are scaled to adjust for dropout

âœ… Pros:

    Reduces overfitting

    Makes model more robust and generalizable

âŒ Cons:

    Slows down training slightly

    Needs careful tuning

âœ… 2. L1 Regularization (Lasso)
ğŸ” What is it?

L1 regularization adds a penalty to the loss function equal to the absolute values of the weights.
Formula:
Loss=Original Loss+Î»âˆ‘âˆ£wâˆ£
Loss=Original Loss+Î»âˆ‘âˆ£wâˆ£

    Î»Î»: regularization strength

    ww: weights of the model

ğŸ¯ Purpose:

To shrink some weights to zero, effectively removing features â†’ this gives a sparse model.
âœ… Pros:

    Good for feature selection

    Makes models simpler

âŒ Cons:

    May be unstable when features are correlated

âœ… 3. L2 Regularization (Ridge)
ğŸ” What is it?

L2 regularization adds a penalty to the loss function equal to the squared values of the weights.
Formula:
Loss=Original Loss+Î»âˆ‘w2
Loss=Original Loss+Î»âˆ‘w2
ğŸ¯ Purpose:

To shrink weights gradually, not to zero â†’ this keeps all features, but makes them smaller.
âœ… Pros:

    Prevents overfitting

    Works well when all features are useful

âŒ Cons:

    Doesnâ€™t eliminate useless features like L1