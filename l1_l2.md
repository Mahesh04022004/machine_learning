✅ 1. Dropout
🔍 What is it?

Dropout is a technique where random neurons are ignored ("dropped") during training.

It means during each training step:

    Some neurons are turned off (their output is set to 0)

    This changes the structure of the network slightly every time

🎯 Purpose:

To prevent overfitting by not letting the model rely too much on specific neurons.
🧠 Simple Analogy:

Imagine a group of students solving problems. If the same 2-3 students always answer, others won’t learn.

So the teacher randomly asks different students each time — this forces all students to participate. That’s dropout in action.
⚙️ How it works:

Let’s say a layer has 100 neurons.

    Dropout rate = 0.2 (20%)

    Then, 20 neurons are randomly disabled during each training iteration

    During testing, all neurons are used, but their outputs are scaled to adjust for dropout

✅ Pros:

    Reduces overfitting

    Makes model more robust and generalizable

❌ Cons:

    Slows down training slightly

    Needs careful tuning

✅ 2. L1 Regularization (Lasso)
🔍 What is it?

L1 regularization adds a penalty to the loss function equal to the absolute values of the weights.
Formula:
Loss=Original Loss+λ∑∣w∣
Loss=Original Loss+λ∑∣w∣

    λλ: regularization strength

    ww: weights of the model

🎯 Purpose:

To shrink some weights to zero, effectively removing features → this gives a sparse model.
✅ Pros:

    Good for feature selection

    Makes models simpler

❌ Cons:

    May be unstable when features are correlated

✅ 3. L2 Regularization (Ridge)
🔍 What is it?

L2 regularization adds a penalty to the loss function equal to the squared values of the weights.
Formula:
Loss=Original Loss+λ∑w2
Loss=Original Loss+λ∑w2
🎯 Purpose:

To shrink weights gradually, not to zero → this keeps all features, but makes them smaller.
✅ Pros:

    Prevents overfitting

    Works well when all features are useful

❌ Cons:

    Doesn’t eliminate useless features like L1